{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32fc243d",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "81b46455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b85f5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db7d0d",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5a1390e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = 'MNIST/digit_xtrain.csv'\n",
    "X_TEST_PATH = 'MNIST/digit_xtest.csv'\n",
    "Y_TRAIN_PATH = 'MNIST/digit_ytrain.csv'\n",
    "Y_TEST_PATH = 'MNIST/digit_ytest.csv'\n",
    "LOGGING_PATH = 'tensorbaord_mnist_digits_logs/'\n",
    "\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "IMAGE_HEIGHT = 28\n",
    "IMAGE_WIDTH = 28\n",
    "CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_HEIGHT*IMAGE_WIDTH*CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bce7bb",
   "metadata": {},
   "source": [
    "# Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "12f63004",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 60 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7787cee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d3062728",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5c184b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 34.2 s\n",
      "Wall time: 35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0dce3612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.77 s\n",
      "Wall time: 5.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadbf344",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "23b4bac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape\n",
    "# 784 => 28(width) * 28(height) * 1(nr. of channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9864f298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all[0]\n",
    "# 0 => very white, 255 => very black,in-between => shades of gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "a69da127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ca74022a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6be50259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081ba04",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a931b5b",
   "metadata": {},
   "source": [
    "#### Re-Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c989e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all, x_test = x_train_all / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d185e06",
   "metadata": {},
   "source": [
    "## Converting Target Values to One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "da606932",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(10)[y_train_all[:5]]\n",
    "# the np.eye() function takes nr. of rows as a parameter and then creates an array with the given nr.\n",
    "# of rows with 1 going diagonally and the rest being zeros. However, in our usage we add an array \n",
    "# of values i.e [y_train_all[:5]], and the value at the position of the index of \n",
    "# the value in [y_train_all[:5]] is replaced with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a81e4bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.eye(NR_CLASSES)[y_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b153d6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.eye(NR_CLASSES)[y_test]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3f94167c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b5c3ca",
   "metadata": {},
   "source": [
    "## Creating Validation Dataset from Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "bd196e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val = x_train_all[:VALIDATION_SIZE]\n",
    "y_val = y_train_all[:VALIDATION_SIZE]\n",
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b774448e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad28d9",
   "metadata": {},
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "7f93ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=[None, TOTAL_INPUTS], name='X')\n",
    "Y = tf.compat.v1.placeholder(tf.float32, shape=[None, NR_CLASSES], name='Labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6945c8c",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "4e2566cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "e84caa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input_, weight_dim, bias_dim, name):\n",
    "    with tf.name_scope(name):\n",
    "        initial_w = tf.compat.v1.truncated_normal(shape=weight_dim, stddev=0.1, seed=42, name='w')\n",
    "        w = tf.Variable(initial_value=initial_w)\n",
    "\n",
    "        initial_b = tf.constant(value=0.0, shape=[bias_dim], name='b')\n",
    "        b = tf.Variable(initial_value=initial_b)\n",
    "\n",
    "        layer_in = tf.matmul(input_, w) + b\n",
    "        if name == 'out':\n",
    "            layer_output = tf.nn.softmax(layer_in)\n",
    "        else:\n",
    "            layer_output = tf.nn.relu(layer_in)\n",
    "        \n",
    "        tf.compat.v1.summary.histogram('weights', w)\n",
    "        tf.compat.v1.summary.histogram('biases', b)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "844f8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_1 = setup_layer(input_=X, weight_dim=[TOTAL_INPUTS, n_hidden1],\n",
    "#                       bias_dim=n_hidden1, name='layer_1')\n",
    "\n",
    "# layer_dropout = tf.compat.v1.nn.dropout(layer_1, keep_prob=0.8)\n",
    "\n",
    "# layer_2 = setup_layer(input_=layer_dropout, weight_dim=[n_hidden1, n_hidden2],\n",
    "#                      bias_dim=n_hidden2, name='layer_2')\n",
    "\n",
    "# output = setup_layer(input_=layer_2, weight_dim=[n_hidden2, NR_CLASSES],\n",
    "#                     bias_dim=NR_CLASSES, name='out')\n",
    "\n",
    "# model_name = f\"{n_hidden1}-{n_hidden2}-DO-E{nr_epochs} LR{learning_rate}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "e5986080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\GREATFAITH CHURCH\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# With DropOut\n",
    "\n",
    "layer_1 = setup_layer(input_=X, weight_dim=[TOTAL_INPUTS, n_hidden1],\n",
    "                      bias_dim=n_hidden1, name='layer_1')\n",
    "\n",
    "layer_dropout = tf.compat.v1.nn.dropout(layer_1, keep_prob=0.8)\n",
    "\n",
    "layer_2 = setup_layer(input_=layer_dropout, weight_dim=[n_hidden1, n_hidden2],\n",
    "                     bias_dim=n_hidden2, name='layer_2')\n",
    "\n",
    "output = setup_layer(input_=layer_2, weight_dim=[n_hidden2, NR_CLASSES],\n",
    "                    bias_dim=NR_CLASSES, name='out')\n",
    "\n",
    "model_name = f\"{n_hidden1}-{n_hidden2}-DO-E{nr_epochs} LR{learning_rate}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1f75f",
   "metadata": {},
   "source": [
    "# Setup Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "19d2bbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directory\n"
     ]
    }
   ],
   "source": [
    "folder_name = f\"{model_name} on {strftime('%d')}-{strftime('%b')} at {strftime('%H')}_{strftime('%M')}\"\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as err:\n",
    "    print(err.strerror)\n",
    "else:\n",
    "    print('Successfully created directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4badadb5",
   "metadata": {},
   "source": [
    "# Loss, Optimisation & Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721f6c1",
   "metadata": {},
   "source": [
    "### Defining Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "c48d9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss_calc'):\n",
    "    loss = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d0178",
   "metadata": {},
   "source": [
    "### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "7321427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf51f6c",
   "metadata": {},
   "source": [
    "### Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "5140de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_calc'):\n",
    "    correct_pred = tf.equal(tf.argmax(output, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "b47ff6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\n",
    "    tf.compat.v1.summary.scalar('cost', loss)\n",
    "    tf.compat.v1.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58fc53",
   "metadata": {},
   "source": [
    "### Check Image Inputs with Tensorbaord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "5fa9db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('show_image'):\n",
    "    x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "    tf.compat.v1.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd14646",
   "metadata": {},
   "source": [
    "# Run  Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b1caf",
   "metadata": {},
   "source": [
    "#### Initialize Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "df0e664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75517d70",
   "metadata": {},
   "source": [
    "#### Setup Filewriter and merge  summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "fefaaa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.compat.v1.summary.merge_all()\n",
    "\n",
    "train_writer = tf.compat.v1.summary.FileWriter(directory + '/train')\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "validation_writer = tf.compat.v1.summary.FileWriter(directory + '/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "95e9f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize global variables\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac8b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9747e199",
   "metadata": {},
   "source": [
    "## Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "d6190178",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "78b00599",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_examples = y_train.shape[0]\n",
    "nr_iterations = int(nr_examples/size_of_batch)\n",
    "\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "edb0d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(batch_size, data, labels):\n",
    "    \n",
    "    global nr_examples\n",
    "    global index_in_epoch\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > nr_examples:\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "    \n",
    "    end = index_in_epoch\n",
    "    \n",
    "    return data[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b909e8",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "21108531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ------------------------------- Training Accuracy 0.8460000157356262\n",
      "Epoch 1 ------------------------------- Training Accuracy 0.859000027179718\n",
      "Epoch 2 ------------------------------- Training Accuracy 0.8669999837875366\n",
      "Epoch 3 ------------------------------- Training Accuracy 0.8690000176429749\n",
      "Epoch 4 ------------------------------- Training Accuracy 0.875\n",
      "Epoch 5 ------------------------------- Training Accuracy 0.8769999742507935\n",
      "Epoch 6 ------------------------------- Training Accuracy 0.9700000286102295\n",
      "Epoch 7 ------------------------------- Training Accuracy 0.9769999980926514\n",
      "Epoch 8 ------------------------------- Training Accuracy 0.9829999804496765\n",
      "Epoch 9 ------------------------------- Training Accuracy 0.9819999933242798\n",
      "Epoch 10 ------------------------------- Training Accuracy 0.9850000143051147\n",
      "Epoch 11 ------------------------------- Training Accuracy 0.9879999756813049\n",
      "Epoch 12 ------------------------------- Training Accuracy 0.9879999756813049\n",
      "Epoch 13 ------------------------------- Training Accuracy 0.9879999756813049\n",
      "Epoch 14 ------------------------------- Training Accuracy 0.9890000224113464\n",
      "Epoch 15 ------------------------------- Training Accuracy 0.9900000095367432\n",
      "Epoch 16 ------------------------------- Training Accuracy 0.9869999885559082\n",
      "Epoch 17 ------------------------------- Training Accuracy 0.9909999966621399\n",
      "Epoch 18 ------------------------------- Training Accuracy 0.9890000224113464\n",
      "Epoch 19 ------------------------------- Training Accuracy 0.9900000095367432\n",
      "Epoch 20 ------------------------------- Training Accuracy 0.9890000224113464\n",
      "Epoch 21 ------------------------------- Training Accuracy 0.9900000095367432\n",
      "Epoch 22 ------------------------------- Training Accuracy 0.9900000095367432\n",
      "Epoch 23 ------------------------------- Training Accuracy 0.9909999966621399\n",
      "Epoch 24 ------------------------------- Training Accuracy 0.9900000095367432\n",
      "Epoch 25 ------------------------------- Training Accuracy 0.9919999837875366\n",
      "Epoch 26 ------------------------------- Training Accuracy 0.9909999966621399\n",
      "Epoch 27 ------------------------------- Training Accuracy 0.9909999966621399\n",
      "Epoch 28 ------------------------------- Training Accuracy 0.9919999837875366\n",
      "Epoch 29 ------------------------------- Training Accuracy 0.9919999837875366\n",
      "Epoch 30 ------------------------------- Training Accuracy 0.9929999709129333\n",
      "Epoch 31 ------------------------------- Training Accuracy 0.9919999837875366\n",
      "Epoch 32 ------------------------------- Training Accuracy 0.9909999966621399\n",
      "Epoch 33 ------------------------------- Training Accuracy 0.9909999966621399\n",
      "Epoch 34 ------------------------------- Training Accuracy 0.9919999837875366\n",
      "Epoch 35 ------------------------------- Training Accuracy 0.9890000224113464\n",
      "Epoch 36 ------------------------------- Training Accuracy 0.9909999966621399\n",
      "Epoch 37 ------------------------------- Training Accuracy 0.9919999837875366\n",
      "Epoch 38 ------------------------------- Training Accuracy 0.9929999709129333\n",
      "Epoch 39 ------------------------------- Training Accuracy 0.9940000176429749\n",
      "Epoch 40 ------------------------------- Training Accuracy 0.9919999837875366\n",
      "Epoch 41 ------------------------------- Training Accuracy 0.9940000176429749\n",
      "Epoch 42 ------------------------------- Training Accuracy 0.9929999709129333\n",
      "Epoch 43 ------------------------------- Training Accuracy 0.9940000176429749\n",
      "Epoch 44 ------------------------------- Training Accuracy 0.9940000176429749\n",
      "Epoch 45 ------------------------------- Training Accuracy 0.9940000176429749\n",
      "Epoch 46 ------------------------------- Training Accuracy 0.9940000176429749\n",
      "Epoch 47 ------------------------------- Training Accuracy 0.9919999837875366\n",
      "Epoch 48 ------------------------------- Training Accuracy 0.9940000176429749\n",
      "Epoch 49 ------------------------------- Training Accuracy 0.9919999837875366\n",
      "Training Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch  in range(nr_epochs):\n",
    "    #------------------------training-------------------------------\n",
    "    for i in range(nr_iterations):\n",
    "        batch_x, batch_y = get_next_batch(batch_size=size_of_batch, data=x_train, labels=y_train)\n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        sess.run(train_step, feed_dict=feed_dictionary)\n",
    "    su, batch_accuracy = sess.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary)\n",
    "    train_writer.add_summary(su, epoch)\n",
    "        \n",
    "    print(f\"Epoch {epoch} ------------------------------- Training Accuracy {batch_accuracy}\")\n",
    "    #--------------------------validation-------------------------------\n",
    "    summary = sess.run(fetches=merged_summary, feed_dict={X:x_val, Y:y_val})\n",
    "    validation_writer.add_summary(summary, epoch)\n",
    "print('Training Finished!')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b57d1",
   "metadata": {},
   "source": [
    "# Reset for Next Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "704f3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "sess.close()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270b466",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "9a7df491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('first_hidden_layer'):\n",
    "#     initial_w1 = tf.compat.v1.truncated_normal(shape=[TOTAL_INPUTS, n_hidden1], stddev=0.1, seed=42, name='w1')\n",
    "#     w1 = tf.Variable(initial_value=initial_w1)\n",
    "\n",
    "#     initial_b1 = tf.constant(value=0.0, shape=[n_hidden1], name='b1')\n",
    "#     b1 = tf.Variable(initial_value=initial_b1)\n",
    "\n",
    "#     layer1_in = tf.matmul(X, w1) + b1\n",
    "#     layer1_out = tf.nn.relu(layer1_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "1e3918a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('second_hidden_layer'):\n",
    "\n",
    "#     initial_w2 = tf.compat.v1.truncated_normal(shape=[n_hidden1, n_hidden2], stddev=0.1, seed=42, name='w2')\n",
    "#     w2 = tf.Variable(initial_value=initial_w2)\n",
    "\n",
    "#     initial_b2 = tf.constant(value=0.0, shape=[n_hidden2], name='b2')\n",
    "#     b2 = tf.Variable(initial_value=initial_b2)\n",
    "\n",
    "#     layer2_in = tf.matmul(layer1_out, w2) + b2\n",
    "#     layer2_out = tf.nn.relu(layer2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "3604b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('output_layer'):\n",
    "#     initial_w3 = tf.compat.v1.truncated_normal(shape=[n_hidden2, NR_CLASSES], stddev=0.1, seed=42, name='w3')\n",
    "#     w3 = tf.Variable(initial_value=initial_w3)\n",
    "\n",
    "#     initial_b3 = tf.constant(value=0.0, shape=[NR_CLASSES], name='b3')\n",
    "#     b3 = tf.Variable(initial_value=initial_b3)\n",
    "\n",
    "#     layer3_in = tf.matmul(layer2_out, w3) + b3\n",
    "#     output = tf.nn.softmax(layer3_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "e0505ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w3.eval(sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
